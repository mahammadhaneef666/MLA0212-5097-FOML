import numpy as np
import random

# Define the environment
class DroneEnv:
    def __init__(self, grid_size, obstacles, start, end):
        self.grid_size = grid_size
        self.obstacles = obstacles
        self.start = start
        self.end = end
        self.reset()

    def reset(self):
        self.position = self.start
        return self.position

    def step(self, action):
        x, y = self.position
        if action == 0:   # up
            y = max(y - 1, 0)
        elif action == 1: # down
            y = min(y + 1, self.grid_size - 1)
        elif action == 2: # left
            x = max(x - 1, 0)
        elif action == 3: # right
            x = min(x + 1, self.grid_size - 1)

        new_position = (x, y)
        reward = -1
        done = False

        if new_position in self.obstacles:
            reward = -100
            new_position = self.position
        elif new_position == self.end:
            reward = 100
            done = True

        self.position = new_position
        return new_position, reward, done

    def render(self):
        grid = np.zeros((self.grid_size, self.grid_size))
        for obs in self.obstacles:
            grid[obs] = -1
        grid[self.end] = 2
        grid[self.position] = 1
        print(grid)

# Q-learning algorithm
class QLearningAgent:
    def __init__(self, state_size, action_size, alpha=0.1, gamma=0.99, epsilon=0.1):
        self.state_size = state_size
        self.action_size = action_size
        self.q_table = np.zeros(state_size + [action_size])
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon

    def choose_action(self, state):
        if random.uniform(0, 1) < self.epsilon:
            return random.randint(0, self.action_size - 1)
        else:
            return np.argmax(self.q_table[state])

    def learn(self, state, action, reward, next_state, done):
        old_value = self.q_table[state][action]
        next_max = np.max(self.q_table[next_state])
        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max * (not done))
        self.q_table[state][action] = new_value

# Set up the environment
grid_size = 5
obstacles = [(1, 1), (1, 2), (2, 2), (3, 3)]
start = (0, 0)
end = (4, 4)
env = DroneEnv(grid_size, obstacles, start, end)

# Set up the agent
state_size = [grid_size, grid_size]
action_size = 4
agent = QLearningAgent(state_size, action_size)

# Train the agent
episodes = 200
for episode in range(episodes):
    state = env.reset()
    state = tuple(state)
    total_reward = 0
    done = False

    while not done:
        action = agent.choose_action(state)
        next_state, reward, done = env.step(action)
        next_state = tuple(next_state)
        agent.learn(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward

    if (episode + 1) % 50 == 0:
        print(f"Episode: {episode + 1}, Total Reward: {total_reward}")

# Test the agent
state = env.reset()
state = tuple(state)
done = False
env.render()
while not done:
    action = agent.choose_action(state)
    next_state, reward, done = env.step(action)
    next_state = tuple(next_state)
    state = next_state
    env.render()
